{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBYpRR8XPNop",
        "colab_type": "text"
      },
      "source": [
        "written by kevin lee\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEoeHR-SNWPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvcyGvntNk-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pytz\n",
        "from datetime import datetime, timezone\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "\n",
        "\n",
        "%cd \"/content/drive/My Drive/TraCAR/darknet2\"\n",
        "cur_dir = %pwd\n",
        "sys.path.append(cur_dir)\n",
        "\n",
        "%cd \"/content/drive/My Drive/TraCAR/toolkit-master/got10k\"\n",
        "cur_dir = %pwd\n",
        "sys.path.append(cur_dir)\n",
        "\n",
        "%cd \"/content/drive/My Drive/TraCAR/toolkit-master/got10k/trackers\"\n",
        "cur_dir = %pwd\n",
        "sys.path.append(cur_dir)\n",
        "\n",
        "%cd \"/content/drive/My Drive/TraCAR/toolkit-master/got10k/utils\"\n",
        "cur_dir = %pwd\n",
        "sys.path.append(cur_dir)\n",
        "\n",
        "%cd \"/content/drive/My Drive/TraCAR/siamrpn-pytorch\"\n",
        "cur_dir = %pwd\n",
        "sys.path.append(cur_dir)\n",
        "\n",
        "from viz import show_frame\n",
        "\n",
        "from trackers import Tracker\n",
        "from siamrpn import TrackerSiamRPN\n",
        "from mosse import mosse\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QscAYYWegCc6",
        "colab_type": "text"
      },
      "source": [
        "# Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxl-OrTWNsyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "siamrpn_path = \"/content/drive/My Drive/TraCAR/siamrpn-pytorch/pretrained/siamrpn/model.pth\"\n",
        "args_darknet_path = \"/content/drive/My Drive/TraCAR/darknet2\"\n",
        "\n",
        "args_confidence = 0.5\n",
        "args_threshold = 0.3\n",
        "\n",
        "eval_train = True\n",
        "eval_test = False\n",
        "eval_val = True\n",
        "\n",
        "args_dataset_path = \"/content/drive/My Drive/TraCAR/datasets/UFPR-ALPR\"\n",
        "gt_dir = os.path.join(args_dataset_path, \"labels\")\n",
        "\n",
        "train_path = os.path.join(args_dataset_path, \"training\")\n",
        "test_path = os.path.join(args_dataset_path, \"testing\")\n",
        "val_path = os.path.join(args_dataset_path, \"validation\")\n",
        "\n",
        "args_vid_path = \"/content/drive/My Drive/TraCAR/datasets/UFPR-ALPR/testing/track0139\"\n",
        "args_img_path = \"/content/drive/My Drive/TraCAR/datasets/UFPR-ALPR/testing/track0150/track0150[01].png\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9iJRDBjOIZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labelsPath = os.path.sep.join([args_darknet_path, \"data\", \"UFPR-ALPR.names\"])\n",
        "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
        "np.random.seed(100)\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
        "\tdtype=\"uint8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fgimgCff_F8",
        "colab_type": "text"
      },
      "source": [
        "# Init Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkl9r66bOMr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# derive the paths to the YOLO weights and model configuration\n",
        "# weightsPath = os.path.sep.join([yolo_path, \"yolov3.weights\"])\n",
        "weightsPath = os.path.sep.join([args_darknet_path, \"KLbackup\",\"UFPR\",\"v12\",\"KLyolov2-tiny-UFPR_12000.weights\"])\n",
        "configPath = os.path.sep.join([args_darknet_path, \"KLbackup\",\"UFPR\",\"v12\",\"Copy of KLyolov2-tiny-UFPR.cfg\"])\n",
        "# load our YOLO object detector trained on UFPR-ALPR\n",
        "print(\"[INFO] loading YOLO from disk...\")\n",
        "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XDL5NrZ2IQv",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOpeF87jf72P",
        "colab_type": "text"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZvBu9LtQ7vU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_and_sort(files_list):\n",
        "  # look through video folder, and filter out txt files. then sort pngs by frame number\n",
        "  filtered_list = []\n",
        "  frame_nums = []\n",
        "  for i in range(len(files_list)):\n",
        "    if files_list[i][-3] != 'p' or files_list[i][-1].isdigit() or files_list[i][0] != 't' or '(' in files_list[i]: \n",
        "      continue\n",
        "    else: # is png file\n",
        "      frame_nums.append(int(files_list[i][-7:-5])) \n",
        "      filtered_list.append(files_list[i])\n",
        "\n",
        "  sorted_indices = sorted(range(len(frame_nums)), key=lambda k: frame_nums[k])\n",
        "\n",
        "  output_list = [filtered_list[sorted_indices[j]] for j in range(len(sorted_indices))]\n",
        "  return output_list\n",
        "\n",
        "def center2corner(box, H=1080, W=1920):\n",
        "  # converts bbox from normalized yolo [x_c, y_c, w, h] to non-normalized [x1, y1, x2, y2]\n",
        "  box[0] = int( (box[0] - box[2]/2)*W )\n",
        "  if box[0] < 0:\n",
        "    box[0] = 0\n",
        "  if box[0] > W:\n",
        "    box[0] = W\n",
        "\n",
        "  box[1] = int( (box[1] - box[1]/2)*H )\n",
        "  if box[0] < 0:\n",
        "    box[0] = 0\n",
        "  if box[0] > H:\n",
        "    box[0] = H\n",
        "\n",
        "  box[2] = (box[0]+box[2]*W)\n",
        "  box[3] = (box[1]+box[3]*H)\n",
        "\n",
        "  return box\n",
        "\n",
        "\n",
        "def IOU(boxA, boxB):\n",
        "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
        "\txA = max(boxA[0], boxB[0])\n",
        "\tyA = max(boxA[1], boxB[1])\n",
        "\txB = min(boxA[2], boxB[2])\n",
        "\tyB = min(boxA[3], boxB[3])\n",
        "\t# compute the area of intersection rectangle\n",
        "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "\t# compute the area of both the prediction and ground-truth\n",
        "\t# rectangles\n",
        "\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "\t# compute the intersection over union by taking the intersection\n",
        "\t# area and dividing it by the sum of prediction + ground-truth\n",
        "\t# areas - the interesection area\n",
        "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\t# return the intersection over union value\n",
        "\treturn iou\n",
        "\n",
        "def eval_image(det_info, gt_det_info, iou_threshold=0.5):\n",
        "  # only consider the detection with the highest confidence\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "  if len(det_info) == 0:\n",
        "    fn += 1\n",
        "  else:\n",
        "    # if det_info[0][0] != gt_det_info[0]: # class ids dont match\n",
        "    #   fp += 1\n",
        "    # else: # class ids match\n",
        "    if True: # dont care about class id\n",
        "      pred_bbox = det_info[0][1]\n",
        "      gt_bbox = gt_det_info[1:5]\n",
        "      pred_bbox[2] += pred_bbox[0]\n",
        "      pred_bbox[3] += pred_bbox[1]\n",
        "\n",
        "      # print('pred {} | gt {}'.format(pred_bbox, gt_bbox))\n",
        "\n",
        "      iou = IOU(pred_bbox, gt_bbox)\n",
        "      if iou >= iou_threshold:\n",
        "        tp += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "  return tp, fp, fn\n",
        "\n",
        "\n",
        "def extract_gt(m_gt_path):\n",
        "  with open(m_gt_path) as fo:\n",
        "    line = fo.readline()\n",
        "    cnt = 1\n",
        "    # print('gt line', line)\n",
        "    while line:\n",
        "      if cnt > 1:\n",
        "        print('ERROR: more than 1 detection in frame. need to rewrite eval function')\n",
        "        sys.exit()\n",
        "\n",
        "      gt_det_info = line.strip().split(' ')\n",
        "      gt_det_info = [float(gt_det_info[i]) for i in range(len(gt_det_info))]\n",
        "      # print(gt_det_info)\n",
        "      gt_det_info[1:5] = center2corner(gt_det_info[1:5])\n",
        "      line = fo.readline()\n",
        "      cnt += 1\n",
        "  return gt_det_info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFHqZsmmdQj-",
        "colab_type": "text"
      },
      "source": [
        "## Prediction Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttMkreaAdTD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect(image_path, Video = None, fig_size=(16,9), show_prediction=False, show_timing=True, show_blank=True, save_blank=True):\n",
        "  det_info = []\n",
        "\n",
        "  image = cv2.imread(image_path)\n",
        "  (H, W) = image.shape[:2]\n",
        "  # determine only the *output* layer names that we need from YOLO\n",
        "  ln = net.getLayerNames()\n",
        "  ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "  # construct a blob from the input image and then perform a forward\n",
        "  # pass of the YOLO object detector, giving us our bounding boxes and\n",
        "  # associated probabilities\n",
        "  blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (512, 288),\n",
        "    swapRB=True, crop=False)\n",
        "  net.setInput(blob)\n",
        "  start = time.time()\n",
        "  layerOutputs = net.forward(ln)\n",
        "  end = time.time()\n",
        "  # show timing information on YOLO\n",
        "  if show_timing:\n",
        "    print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))\n",
        "\n",
        "  # initialize our lists of detected bounding boxes, confidences, and\n",
        "  # class IDs, respectively\n",
        "  boxes = []\n",
        "  confidences = []\n",
        "  classIDs = []\n",
        "\n",
        "  # loop over each of the layer outputs\n",
        "  for output in layerOutputs:\n",
        "    # loop over each of the detections\n",
        "    for detection in output:\n",
        "      # extract the class ID and confidence (i.e., probability) of\n",
        "      # the current object detection\n",
        "      scores = detection[5:]\n",
        "      classID = np.argmax(scores)\n",
        "      confidence = scores[classID]\n",
        "      # filter out weak predictions by ensuring the detected\n",
        "      # probability is greater than the minimum probability\n",
        "      if confidence > args_confidence:\n",
        "        # scale the bounding box coordinates back relative to the\n",
        "        # size of the image, keeping in mind that YOLO actually\n",
        "        # returns the center (x, y)-coordinates of the bounding\n",
        "        # box followed by the boxes' width and height\n",
        "        box = detection[0:4] * np.array([W, H, W, H])\n",
        "        (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "        # use the center (x, y)-coordinates to derive the top and\n",
        "        # and left corner of the bounding box\n",
        "        x = int(centerX - (width / 2))\n",
        "        y = int(centerY - (height / 2))\n",
        "        # update our list of bounding box coordinates, confidences,\n",
        "        # and class IDs\n",
        "        boxes.append([x, y, int(width), int(height)])\n",
        "        confidences.append(float(confidence))\n",
        "        classIDs.append(classID)\n",
        "    \n",
        "  # apply non-maxima suppression to suppress weak, overlapping bounding\n",
        "  # boxes\n",
        "  idxs = cv2.dnn.NMSBoxes(boxes, confidences, args_confidence, args_threshold)\n",
        "\n",
        "  best_confidence = 0\n",
        "  # ensure at least one detection exists\n",
        "  if len(idxs) > 0:\n",
        "    # loop over the indexes we are keeping\n",
        "    for i in idxs.flatten():\n",
        "      # extract the bounding box coordinates\n",
        "      (x, y) = (boxes[i][0], boxes[i][1])\n",
        "      (w, h) = (boxes[i][2], boxes[i][3])\n",
        "      # draw a bounding box rectangle and label on the image\n",
        "      color = [int(c) for c in COLORS[classIDs[i]]]\n",
        "      cv2.rectangle(image, (x, y), (x + w, y + h), color, 6)\n",
        "      text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
        "      cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        2.5, color, 6)\n",
        "      \n",
        "      if confidences[i] > best_confidence:\n",
        "        det_info = [classIDs[i], boxes[i]], confidences[i]\n",
        "        best_confidence = confidences[i]\n",
        "  \n",
        "  #save frames to videos  \n",
        "  if Video is not None and save_blank:\n",
        "    Video.write(image)\n",
        "\n",
        "\n",
        "  # show the output image\n",
        "  if show_prediction and (len(det_info) != 0 or show_blank):\n",
        "    plt.figure(figsize = fig_size) #change figure size here. native aspect ratio is 16:9\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "\n",
        "  return det_info\n",
        "\n",
        "def detect_track_mosse(vid_path, Video = None, N=5, show_prediction=True, show_timing=True, show_blank=False):\n",
        "  output_bbox = False\n",
        "  class Args:\n",
        "    lr = 0.00125\n",
        "    sigma = 0.1\n",
        "    num_pretrain = 64\n",
        "    rotate = True\n",
        "    record = False\n",
        "\n",
        "  args = Args()\n",
        "  bounding_box_list = []\n",
        "\n",
        "\n",
        "  frame = 0\n",
        "  frame_temp = 0\n",
        "  det_info_temp = []\n",
        "  while frame < 30:\n",
        "    # print('frame={}, frame_temp={}'.format(frame, frame_temp))\n",
        "    for root, dirs, files in os.walk(vid_path):\n",
        "      img_list = filter_and_sort(files)\n",
        "      \n",
        "      # repeat = False\n",
        "      if frame_temp%N == 0:\n",
        "        m_img_path = os.path.join(root, img_list[frame])\n",
        "      \n",
        "\n",
        "        det_info = detect(m_img_path,Video,fig_size=(15,4), show_prediction=show_prediction, show_timing=show_timing, show_blank=show_blank)\n",
        "       \n",
        "\n",
        "        if (len(det_info) == 0 and len(det_info_temp) != 0):\n",
        "          # print('No detection, using prev det as template for tracking for cur frame')\n",
        "          prev_det_bbox = det_info_temp[0][1]\n",
        "          x = prev_det_bbox[0]\n",
        "          y = prev_det_bbox[1]\n",
        "          width = prev_det_bbox[2]\n",
        "          height = prev_det_bbox[3]\n",
        "          tracker = mosse(args, args_vid_path)\n",
        "\n",
        "          # sometimes has fft size errors\n",
        "          try:\n",
        "            tracker.start_tracking((x, y, width, height), frame, frame+1, bounding_box_list, False, Video, show_prediction=show_prediction,show_timing=show_timing)\n",
        "            bbox = bounding_box_list[-1]\n",
        "          except:\n",
        "            bounding_box_list.append([])\n",
        "          \n",
        "          # repeat = True\n",
        "        elif len(det_info) != 0:\n",
        "          # print('Detected')\n",
        "          det_info_temp = det_info\n",
        "          bbox = det_info[0][1]\n",
        "          bounding_box_list.append(bbox)\n",
        "\n",
        "          x = bbox[0]\n",
        "          y = bbox[1]\n",
        "          width = bbox[2]\n",
        "          height = bbox[3]\n",
        "\n",
        "        else:\n",
        "          # print('No detections found so far')\n",
        "          bounding_box_list.append([])\n",
        "          frame_temp -= 1\n",
        "        \n",
        "        frame += 1\n",
        "        frame_temp += 1\n",
        "      else:\n",
        "        tracker = mosse(args, args_vid_path)\n",
        "        try:\n",
        "          # print('2a len(bounding_box_list)={}'.format(len(bounding_box_list)))\n",
        "          tracker.start_tracking((x, y, width, height), frame, frame+N-1, bounding_box_list, output_bbox,Video, show_prediction=show_prediction,show_timing=show_timing)\n",
        "          frame += (N-1)\n",
        "          frame_temp += N-1 \n",
        "        except:\n",
        "          # print('2b len(bounding_box_list)={}'.format(len(bounding_box_list)))\n",
        "          bounding_box_list.append([])\n",
        "          frame += 1\n",
        "          frame_temp += 1\n",
        "\n",
        "\n",
        "  return bounding_box_list\n",
        "\n",
        "def detect_track_siamrpn(model_path, vid_path, Video=None, N=5, show_prediction=True, show_timing=True, show_blank=False):\n",
        "  ### assume that siamrpn always detects an object\n",
        "\n",
        "  tracker = TrackerSiamRPN(net_path=model_path)\n",
        "  bounding_box_list = []\n",
        "  template = []\n",
        "  template_img = []\n",
        "  frame = 0\n",
        "  repeat = False\n",
        "\n",
        "  for root, dirs, files in os.walk(vid_path):\n",
        "    img_list = filter_and_sort(files)\n",
        "    img_path_list = [os.path.join(root, img_list[i]) for i in range(len(img_list))]\n",
        "    # print(img_path_list)\n",
        "    while frame < len(img_path_list):\n",
        "      # print('frame ',frame+1)\n",
        "      m_img_path = img_path_list[frame]\n",
        "      if frame%N == 0 or repeat: # do detection\n",
        "        repeat = False\n",
        "        det_info = detect(m_img_path,Video, fig_size=(15,4), show_prediction=show_prediction, show_timing=show_timing, show_blank=show_blank, save_blank=False)\n",
        "        if len(det_info) != 0: # update template\n",
        "          template = det_info[0][1]\n",
        "          template_img_path = [m_img_path]\n",
        "          # print(template)\n",
        "          bounding_box_list.append(template)\n",
        "        else: # did not detect any object, then do tracking with template\n",
        "          if len(template) != 0: # if we have a valid template\n",
        "            \n",
        "            img_files = template_img_path + [img_path_list[frame]]\n",
        "            boxes, times = tracker.track(img_files, template, visualize=show_prediction, Video = Video)\n",
        "\n",
        "            for idx, mbox in enumerate(boxes):\n",
        "              if idx > 0:\n",
        "                bounding_box_list.append([mbox[0], mbox[1], mbox[2], mbox[3]])\n",
        "          else: # did not find any detections, and no template\n",
        "            bounding_box_list.append([])\n",
        "        frame += 1\n",
        "      else: # do tracking for N-1 frames\n",
        "        end_frame = frame+N-1\n",
        "        if end_frame > len(img_path_list):\n",
        "          end_frame = len(img_path_list)\n",
        "\n",
        "        # print(img_path_list[frame:end_frame])\n",
        "        if len(template) != 0: # if we have valid template\n",
        "          img_files = template_img_path + img_path_list[frame:end_frame]\n",
        "          boxes, times = tracker.track(img_files, template, visualize=show_prediction, Video = Video)\n",
        "          for idx, mbox in enumerate(boxes):\n",
        "            if idx > 0:\n",
        "              bounding_box_list.append([mbox[0], mbox[1], mbox[2], mbox[3]])\n",
        "          frame += N-1 \n",
        "        else: # no template stored, then cannot do tracking\n",
        "          repeat = True\n",
        "  if len(bounding_box_list) != len(img_path_list):\n",
        "    print('ERROR: only recorded {} bounding boxes'.format(len(bounding_box_list)))\n",
        "    sys.exit()\n",
        "  return bounding_box_list\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGscmCKGdl6u",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ38iEWIdoqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(path, print_every=10, print_vid_metrics=False):\n",
        "  TP = 0\n",
        "  FP = 0\n",
        "  FN = 0\n",
        "\n",
        "  num_vids = 0\n",
        "  total_vids = 0\n",
        "\n",
        "  utc_dt = datetime.now(timezone.utc)\n",
        "  PST = pytz.timezone('US/Pacific')\n",
        "  print(\"\\nPacific time {}\\n\".format(utc_dt.astimezone(PST).isoformat()))\n",
        "  start = time.time()\n",
        "\n",
        "  for root1, dirs, _ in os.walk(path):\n",
        "    total_vids = len(dirs)\n",
        "    for mdir in dirs:\n",
        "      vid_tp = 0\n",
        "      vid_fp = 0\n",
        "      vid_fn = 0\n",
        "      for root2, _, images in os.walk(os.path.join(root1, mdir)):\n",
        "        img_list = filter_and_sort(images)\n",
        "        for m_img in img_list:\n",
        "          if m_img[-1].isdigit():\n",
        "            continue\n",
        "          m_img_path = os.path.join(root2, m_img)\n",
        "\n",
        "          # Run detection on current image\n",
        "          det_info = detect(m_img_path, show_prediction=False, show_timing=False)\n",
        "\n",
        "          # compare boxes with GT\n",
        "          m_gt_path = os.path.join(gt_dir, m_img[0:-4]+'.txt')\n",
        "          gt_det_info = extract_gt(m_gt_path)\n",
        "\n",
        "          tp, fp, fn = eval_image(det_info, gt_det_info)\n",
        "\n",
        "          vid_tp += tp\n",
        "          vid_fp += fp\n",
        "          vid_fn += fn\n",
        "\n",
        "      TP += vid_tp\n",
        "      FP += vid_fp\n",
        "      FN += vid_fn\n",
        "\n",
        "      if vid_tp == 0:\n",
        "        vid_precision = 0\n",
        "        vid_recall = 0\n",
        "      else:\n",
        "        vid_precision = vid_tp/(vid_tp + vid_fp)\n",
        "        vid_recall = vid_tp/(vid_tp + vid_fn)\n",
        "\n",
        "      if print_vid_metrics:\n",
        "        print('[INFO] Video {} has precision = {} and recall = {}'.format(mdir, vid_precision, vid_recall))\n",
        "\n",
        "      num_vids += 1\n",
        "      if num_vids % print_every == 0:\n",
        "        print('[INFO] Processed ', num_vids, ' videos out of ', total_vids, 'videos | minutes elapsed from start = ', int((time.time()-start)/60),'\\n')\n",
        "\n",
        "  precision = TP/(TP+FP)\n",
        "  recall = TP/(TP+FN)\n",
        "\n",
        "  print('[INFO] Precision = ', precision, ' | Recall = ', recall, ' at Confidence = ', args_confidence)\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"[INFO] EVAL took {} minutes\".format( int((end - start)/60)) )\n",
        "\n",
        "  utc_dt = datetime.now(timezone.utc)\n",
        "  PST = pytz.timezone('US/Pacific')\n",
        "  print(\"\\nPacific time {}\\n\".format(utc_dt.astimezone(PST).isoformat()))\n",
        "\n",
        "  return precision, recall\n",
        "\n",
        "\n",
        "def evaluate_model_mosse(path, N=5, print_every=10, print_vid_metrics=False):\n",
        "  TP = 0\n",
        "  FP = 0\n",
        "  FN = 0\n",
        "\n",
        "  num_vids = 0\n",
        "  total_vids = 0\n",
        "\n",
        "  utc_dt = datetime.now(timezone.utc)\n",
        "  PST = pytz.timezone('US/Pacific')\n",
        "  print(\"\\nPacific time {}\\n\".format(utc_dt.astimezone(PST).isoformat()))\n",
        "  start = time.time()\n",
        "\n",
        "  for root1, dirs, _ in os.walk(path):\n",
        "    total_vids = len(dirs)\n",
        "    for mdir in dirs:\n",
        "\n",
        "      if mdir == 'track0095':\n",
        "        print('skip track0095')\n",
        "        continue\n",
        "\n",
        "      vid_tp = 0\n",
        "      vid_fp = 0\n",
        "      vid_fn = 0\n",
        "      full_vid_path = os.path.join(root1, mdir)\n",
        "      bboxes = detect_track_mosse(full_vid_path, N=N, show_prediction=False, show_timing=False)\n",
        "      for root2, _, images in os.walk(os.path.join(root1, mdir)):\n",
        "        img_list = filter_and_sort(images)\n",
        "        for frame, m_img in enumerate(img_list):\n",
        "          if m_img[-1].isdigit():\n",
        "            continue\n",
        "          m_img_path = os.path.join(root2, m_img)\n",
        "          m_gt_path = os.path.join(gt_dir, m_img[0:-4]+'.txt')\n",
        "          gt_det_info = extract_gt(m_gt_path)\n",
        "          \n",
        "          det_info = bboxes[frame]\n",
        "          if len(det_info) != 0:\n",
        "            det_info = [[0, det_info]]\n",
        "          tp, fp, fn = eval_image(det_info, gt_det_info)\n",
        "          vid_tp += tp\n",
        "          vid_fp += fp\n",
        "          vid_fn += fn\n",
        "\n",
        "      TP += vid_tp\n",
        "      FP += vid_fp\n",
        "      FN += vid_fn\n",
        "\n",
        "      if vid_tp == 0:\n",
        "        vid_precision = 0\n",
        "        vid_recall = 0\n",
        "      else:\n",
        "        vid_precision = vid_tp/(vid_tp + vid_fp)\n",
        "        vid_recall = vid_tp/(vid_tp + vid_fn)\n",
        "\n",
        "      if print_vid_metrics:\n",
        "        print('[INFO] Video {} has precision = {} and recall = {}'.format(mdir, vid_precision, vid_recall))\n",
        "\n",
        "      num_vids += 1\n",
        "      if num_vids % print_every == 0:\n",
        "        print('[INFO] Processed ', num_vids, ' videos out of ', total_vids, 'videos | minutes elapsed from start = ', int((time.time()-start)/60),'\\n')\n",
        "\n",
        "  precision = TP/(TP+FP)\n",
        "  recall = TP/(TP+FN)\n",
        "\n",
        "  print('[INFO] Precision = ', precision, ' | Recall = ', recall, ' at Confidence = ', args_confidence)\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"[INFO] EVAL took {} minutes\".format( int((end - start)/60)) )\n",
        "\n",
        "  utc_dt = datetime.now(timezone.utc)\n",
        "  PST = pytz.timezone('US/Pacific')\n",
        "  print(\"\\nPacific time {}\\n\".format(utc_dt.astimezone(PST).isoformat()))\n",
        "\n",
        "  return precision, recall\n",
        "\n",
        "def evaluate_model_siamrpn(model_path,path, N=5, print_every=10, print_vid_metrics=False, Video = None):\n",
        "  TP = 0\n",
        "  FP = 0\n",
        "  FN = 0\n",
        "\n",
        "  num_vids = 0\n",
        "  total_vids = 0\n",
        "\n",
        "  utc_dt = datetime.now(timezone.utc)\n",
        "  PST = pytz.timezone('US/Pacific')\n",
        "  print(\"\\nPacific time {}\\n\".format(utc_dt.astimezone(PST).isoformat()))\n",
        "  start = time.time()\n",
        "\n",
        "  for root1, dirs, _ in os.walk(path):\n",
        "    total_vids = len(dirs)\n",
        "    for mdir in dirs:\n",
        "      vid_tp = 0\n",
        "      vid_fp = 0\n",
        "      vid_fn = 0\n",
        "      full_vid_path = os.path.join(root1, mdir)\n",
        "\n",
        "      if mdir == 'track0095': #this video is missing frame [03], so just skip whole vid\n",
        "        print('skip track0095')\n",
        "        continue\n",
        "      \n",
        "      bboxes = detect_track_siamrpn(model_path, full_vid_path, N=N, show_prediction=False, show_timing=False)\n",
        "  \n",
        "      for root2, _, images in os.walk(os.path.join(root1, mdir)):\n",
        "        img_list = filter_and_sort(images)\n",
        "        for frame, m_img in enumerate(img_list):\n",
        "          if m_img[-1].isdigit():\n",
        "            continue\n",
        "          m_img_path = os.path.join(root2, m_img)\n",
        "          m_gt_path = os.path.join(gt_dir, m_img[0:-4]+'.txt')\n",
        "    \n",
        "          gt_det_info = extract_gt(m_gt_path)\n",
        "          \n",
        "\n",
        "          det_info = bboxes[frame]\n",
        "\n",
        "          if len(det_info) != 0:\n",
        "            det_info = [[0, det_info]]\n",
        "          tp, fp, fn = eval_image(det_info, gt_det_info)\n",
        "          vid_tp += tp\n",
        "          vid_fp += fp\n",
        "          vid_fn += fn\n",
        "\n",
        "      TP += vid_tp\n",
        "      FP += vid_fp\n",
        "      FN += vid_fn\n",
        "\n",
        "      if vid_tp == 0:\n",
        "        vid_precision = 0\n",
        "        vid_recall = 0\n",
        "      else:\n",
        "        vid_precision = vid_tp/(vid_tp + vid_fp)\n",
        "        vid_recall = vid_tp/(vid_tp + vid_fn)\n",
        "\n",
        "      if print_vid_metrics:\n",
        "        print('[INFO] Video {} has precision = {} and recall = {}'.format(mdir, vid_precision, vid_recall))\n",
        "\n",
        "      num_vids += 1\n",
        "      if num_vids % print_every == 0:\n",
        "        print('[INFO] Processed ', num_vids, ' videos out of ', total_vids, 'videos | minutes elapsed from start = ', int((time.time()-start)/60),'\\n')\n",
        "\n",
        "  if TP == 0:\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "  else:\n",
        "    precision = TP/(TP+FP)\n",
        "    recall = TP/(TP+FN)\n",
        "\n",
        "  print('[INFO] Precision = ', precision, ' | Recall = ', recall, ' at Confidence = ', args_confidence)\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"[INFO] EVAL took {} minutes\".format( int((end - start)/60)) )\n",
        "\n",
        "  utc_dt = datetime.now(timezone.utc)\n",
        "  PST = pytz.timezone('US/Pacific')\n",
        "  print(\"\\nPacific time {}\\n\".format(utc_dt.astimezone(PST).isoformat()))\n",
        "\n",
        "  return precision, recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHehx3PWgF-P",
        "colab_type": "text"
      },
      "source": [
        "# Detection on single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in6LKBX9Ogie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "detect(args_img_path, fig_size=(15,5), show_prediction=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdmAM-HNgJk0",
        "colab_type": "text"
      },
      "source": [
        "# Detection on single video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daIeWNwEOtPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "\n",
        "out = cv2.VideoWriter('/content/drive/My Drive/TraCAR/track139_detonly.mp4',fourcc, 5, (1920, 1080))\n",
        "\n",
        "frame = 0\n",
        "for root, dirs, files in os.walk(args_vid_path):\n",
        "  img_list = filter_and_sort(files)\n",
        "\n",
        "  for m_img in img_list:\n",
        "\n",
        "    m_img_path = os.path.join(root, m_img)\n",
        "\n",
        "    det_info = detect(m_img_path, out, fig_size=(15,4), show_prediction=True)\n",
        "    # print(det_info)\n",
        "\n",
        "    print('frame ', frame)\n",
        "    \n",
        "    #save each frame into some path\n",
        "\n",
        "    frame += 1 \n",
        "out.release()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFYYEzUXCmWR",
        "colab_type": "text"
      },
      "source": [
        "# detection + MOSSE\n",
        "github link here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POV3hECGC48J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv2.VideoWriter('/content/drive/My Drive/TraCAR/track139_det+mosse_N=5.mp4',fourcc, 5, (1920, 1080))\n",
        "boxes = detect_track_mosse(args_vid_path,out,N=5)\n",
        "out.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nakt7Av31RB8",
        "colab_type": "text"
      },
      "source": [
        "#detection + siamrpn\n",
        "https://github.com/huanglianghua/siamrpn-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIxBwTRogrf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv2.VideoWriter('/content/drive/My Drive/TraCAR/track139_det+siamrpn_N=5.mp4',fourcc, 5, (1920, 1080))\n",
        "# show_prediction needs to be True to save video result\n",
        "boxes = detect_track_siamrpn(siamrpn_path, args_vid_path, out, N=5, show_prediction=True) \n",
        "out.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZNBUK9cy73V",
        "colab_type": "text"
      },
      "source": [
        "#Evaluate detection only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxshy-UjAv6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision, recall = evaluate_model(train_path, print_every=10, print_vid_metrics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diE8Iy3hEAHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision, recall = evaluate_model(test_path, print_every=10, print_vid_metrics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5t3InF_EQhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision, recall = evaluate_model(val_path, print_every=5, print_vid_metrics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1dlmvIuUGRh",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate Detection + MOSSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "06jMoP1v9Fza",
        "colab": {}
      },
      "source": [
        "precision, recall = evaluate_model_mosse(train_path, N=10, print_every=10, print_vid_metrics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SrAX1F32Ul3T",
        "colab": {}
      },
      "source": [
        "precision, recall = evaluate_model_mosse(test_path, N=10, print_every=10, print_vid_metrics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzp6lOiDUFj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision, recall = evaluate_model_mosse(val_path, N=10, print_every=5, print_vid_metrics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjbPVs2pBS--",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate Detection+SiamRPN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha2RR4DHBmM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision, recall = evaluate_model_siamrpn(siamrpn_path, train_path, N=10, print_every=10, print_vid_metrics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoejlMHEBmfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision, recall = evaluate_model_siamrpn(siamrpn_path, test_path, N=10, print_every=10, print_vid_metrics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXuKmdxFBmqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision, recall = evaluate_model_siamrpn(siamrpn_path, val_path, N=10, print_every=5, print_vid_metrics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}